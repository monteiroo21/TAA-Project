{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tropical Cyclone Rainfall Prediction: Autoencoder Model Implementation\n",
    "\n",
    "This notebook demonstrates the implementation, training, and evaluation of an Autoencoder model for the task of translating Infrared (IR) satellite imagery to Passive Microwave Rainfall (PMR) estimations. The core idea is to learn a compressed representation of the input IR image and then reconstruct the corresponding PMR image.\n",
    "\n",
    "We will cover:\n",
    "1.  **Setup and Global Parameters**: Importing necessary libraries and defining constants.\n",
    "2.  **Data Preprocessing**: Functions for loading, augmenting, resizing, and normalizing image pairs.\n",
    "3.  **Dataset Preparation**: Creating TensorFlow `tf.data.Dataset` objects for training and testing.\n",
    "4.  **Autoencoder Architecture**: Defining the custom Autoencoder model using a U-Net-like structure.\n",
    "5.  **Model Instantiation and Training**: Setting up the model, defining callbacks, and initiating the training process.\n",
    "6.  **Model Testing and Visualization**: Evaluating the trained model and visualizing its predictions against ground truth images.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Setup and Global Parameters\n",
    "\n",
    "We begin by importing all necessary libraries, including TensorFlow for building the neural network, NumPy for numerical operations, Matplotlib for plotting, and Pandas for data manipulation. We also set up global parameters like image dimensions, batch sizes, and activation functions for experimentation. `scienceplots` is used for scientific-style plots."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.callbacks import TensorBoard, CSVLogger\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from keras.datasets import mnist # This import seems unused in the provided code\n",
    "from itertools import product # This import seems unused in the provided code\n",
    "from tensorflow.data import AUTOTUNE\n",
    "import os\n",
    "import math\n",
    "import random # This import seems unused in the provided code\n",
    "import scienceplots\n",
    "\n",
    "# Apply a scientific plotting style for better visualization\n",
    "plt.style.use('science')\n",
    "\n",
    "# Define hyperparameter choices for potential experimentation (though only one choice is run here)\n",
    "batch_sizes = (4, 8, 16, 32, 64)\n",
    "last_activations = (\"linear\", \"relu\", \"sigmoid\", \"tanh\")\n",
    "\n",
    "# Chosen parameters for the current run\n",
    "chosen_parameters = [64, \"linear\"] \n",
    "num_epochs = 20\n",
    "\n",
    "print(f\"Running test with parameters: {chosen_parameters}\")\n",
    "\n",
    "# Assign chosen parameters to variables for model configuration\n",
    "batch_size = int(chosen_parameters[0])\n",
    "last_activation = chosen_parameters[1]\n",
    "\n",
    "# Define dataset buffer size and batch size\n",
    "DATASET_BUFFER_SIZE = 1000\n",
    "DATASET_BATCH_SIZE = batch_size\n",
    "\n",
    "# Define target image dimensions\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "The `preprocess_pair` function handles loading, augmenting, resizing, and normalizing the image data. Each input JPG file is expected to contain two images side-by-side: the \"real\" (PMR) image on the left and the \"input\" (IR) image on the right.\n",
    "\n",
    "-   **Loading and Splitting**: Reads the JPEG image as grayscale (`channels=1`) and splits it into `real` (left half) and `inp` (right half).\n",
    "-   **Augmentation (Training Only)**: During training, `RandomRotation`, `RandomZoom`, `RandomContrast`, and `RandomBrightness` augmentations are applied. Crucially, the input and real images are concatenated *before* augmentation and then split back to ensure identical transformations.\n",
    "-   **Resizing**: Both images are resized to `IMG_WIDTH` x `IMG_HEIGHT` using bilinear interpolation and reflective padding to avoid artifacts.\n",
    "-   **Normalization**: Z-score normalization is applied to each image independently to standardize pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 1) Specific augmentation layers defined as a Sequential model\n",
    "data_augment = tf.keras.Sequential([\n",
    "    # Random rotations between 0 and 360 degrees (factor 1.0 means full circle)\n",
    "    layers.RandomRotation(factor=1.0, fill_mode='reflect'),\n",
    "    # Light zoom in/out (Â±10%)\n",
    "    layers.RandomZoom(0.1, 0.1, fill_mode='reflect'),\n",
    "    # Light contrast variations\n",
    "    layers.RandomContrast(0.05),\n",
    "    # Light brightness variations\n",
    "    layers.RandomBrightness(0.05),\n",
    "])\n",
    "\n",
    "def preprocess_pair(path, training):\n",
    "    # a) Load and split the image\n",
    "    # Decode JPEG to grayscale (1 channel for intensity)\n",
    "    img = tf.io.decode_jpeg(tf.io.read_file(path), channels=1)\n",
    "    w = tf.shape(img)[1] // 2 # Calculate half width to split\n",
    "    \n",
    "    # \"real\" (PMR) is the left half, \"inp\" (IR) is the right half\n",
    "    real = img[:, :w, :]\n",
    "    inp  = img[:, w:, :]\n",
    "\n",
    "    # b) Apply augmentation only during training\n",
    "    if training:\n",
    "        # Concatenate to apply identical transformations to both images\n",
    "        pair = tf.concat([inp, real], axis=2)  # shape (H, W, 2)\n",
    "        pair = data_augment(pair) # Apply defined augmentations\n",
    "        inp, real = tf.split(pair, num_or_size_splits=2, axis=2) # Split back\n",
    "\n",
    "    # c) Resize with bilinear interpolation + reflective padding to avoid artifacts\n",
    "    inp  = tf.image.resize(inp,  [IMG_WIDTH, IMG_HEIGHT], method='bilinear')\n",
    "    real = tf.image.resize(real, [IMG_WIDTH, IMG_HEIGHT], method='bilinear')\n",
    "\n",
    "    # d) Z-score normalization per image (standardize pixel values)\n",
    "    inp_mean, inp_var = tf.nn.moments(inp, axes=[0,1])\n",
    "    real_mean, real_var = tf.nn.moments(real, axes=[0,1])\n",
    "    inp  = (inp  - inp_mean)  / tf.sqrt(inp_var  + 1e-6) # Add epsilon for numerical stability\n",
    "    real = (real - real_mean) / tf.sqrt(real_var + 1e-6)\n",
    "\n",
    "    return inp, real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### 3. Dataset Preparation\n",
    "\n",
    "The `make_ds` function creates optimized TensorFlow `Dataset` objects for efficient data loading during training and testing.\n",
    "\n",
    "-   **List Files**: Lists all image files matching the given pattern.\n",
    "-   **Shuffling (Training Only)**: Shuffles the dataset for training to ensure randomness and prevent overfitting to data order.\n",
    "-   **Mapping**: Applies the `preprocess_pair` function to each file in parallel using `AUTOTUNE` for optimized performance.\n",
    "-   **Batching**: Batches the processed images into `DATASET_BATCH_SIZE`. `drop_remainder=True` ensures all batches have the same size.\n",
    "-   **Prefetching**: `prefetch(AUTOTUNE)` allows the data pipeline to fetch new batches in the background while the model is training on the current batch, minimizing idle time."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def make_ds(pattern, training):\n",
    "    ds = tf.data.Dataset.list_files(pattern)\n",
    "    if training:\n",
    "        ds = ds.shuffle(DATASET_BUFFER_SIZE) # Shuffle for training dataset\n",
    "    ds = ds.map(lambda p: preprocess_pair(p, training),\n",
    "                num_parallel_calls=AUTOTUNE) # Map preprocessing function in parallel\n",
    "    ds = ds.batch(DATASET_BATCH_SIZE, drop_remainder=True) # Batch data\n",
    "    ds = ds.prefetch(AUTOTUNE) # Prefetch for optimized pipeline\n",
    "    return ds\n",
    "\n",
    "# Create training and test datasets\n",
    "train_dataset = make_ds('./data/TCIRRP/train1k/*.jpg', training=True)\n",
    "test_dataset  = make_ds('./data/TCIRRP/test0.1k/*.jpg',  training=False) # Smaller test set for validation\n",
    "test_dataset_full = make_ds('./data/TCIRRP/test/*.jpg', training=False) # Full test set for final evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### 4. Autoencoder Architecture\n",
    "\n",
    "The `Autoencoder` class defines the neural network architecture. It consists of an `encoder` (to compress the input) and a `decoder` (to reconstruct the image). This architecture is influenced by the U-Net design, focusing on hierarchical feature learning.\n",
    "\n",
    "-   **Encoder**:\n",
    "    -   Takes a `(IMG_WIDTH, IMG_HEIGHT, 1)` grayscale image as input.\n",
    "    -   Comprises three `Conv2D` layers. Each layer uses a 3x3 kernel, `strides=2` (for downsampling), `padding='same'`, `BatchNormalization` (for stable training), and `ReLU` activation.\n",
    "    -   The number of filters increases with depth: 32 -> 64 -> 128, capturing increasingly complex features.\n",
    "    -   The output is `Flatten`ed and passed through a `Dense` layer to create the `LATENT_DIMENSIONS` (32) bottleneck.\n",
    "\n",
    "-   **Decoder**:\n",
    "    -   Takes the `LATENT_DIMENSIONS` vector as input.\n",
    "    -   Starts with a `Dense` layer to project the latent vector back to the dimensions suitable for reshaping into a 3D tensor (`self.bw * self.bh * self.bc`).\n",
    "    -   `Reshape`s the output to the bottleneck spatial dimensions.\n",
    "    -   Uses two `Conv2DTranspose` layers (also known as deconvolution) for upsampling. These layers mirror the encoder's downsampling, increasing spatial dimensions with `strides=2`.\n",
    "    -   The number of filters decreases: 64 -> 32. Each layer is followed by `BatchNormalization` and `ReLU`.\n",
    "    -   The final `Conv2DTranspose` layer outputs a single channel image of the original size, with `last_activation` (e.g., 'linear' for continuous output) applied."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "LATENT_DIMENSIONS = 32 # Dimension of the compressed latent space\n",
    "    \n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        \n",
    "        # Encoder: Applies convolutional layers to compress input to latent space\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1)), # Input shape: grayscale image\n",
    "            \n",
    "            # First Convolutional Block: Downsamples and increases filters\n",
    "            layers.Conv2D(32, 3, strides=2, padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(), # Normalizes activations for stable training\n",
    "            layers.Activation('relu'),   # Rectified Linear Unit activation\n",
    "            \n",
    "            # Second Convolutional Block\n",
    "            layers.Conv2D(64, 3, strides=2, padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            \n",
    "            # Third Convolutional Block\n",
    "            layers.Conv2D(128, 3, strides=2, padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "    \n",
    "            layers.Flatten(), # Flattens the 3D feature maps to a 1D vector\n",
    "            layers.Dense(LATENT_DIMENSIONS, activation='relu'), # Maps to the latent space\n",
    "        ])\n",
    "        \n",
    "        # Calculate bottleneck dimensions for reshaping in the decoder\n",
    "        self.bw = math.ceil(IMG_WIDTH / 2**3)   # Width after 3 stride-2 convolutions\n",
    "        self.bh = math.ceil(IMG_HEIGHT / 2**3)  # Height after 3 stride-2 convolutions\n",
    "        self.bc = 128                           # Channels at the bottleneck (from last encoder layer)\n",
    "                    \n",
    "        # Decoder: Reconstructs image from latent space\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(LATENT_DIMENSIONS,)), # Input is the latent vector\n",
    "            \n",
    "            # Dense layer to expand latent vector before reshaping\n",
    "            layers.Dense(self.bw * self.bh * self.bc, use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            \n",
    "            layers.Reshape((self.bw, self.bh, self.bc)), # Reshape to 3D tensor for transposed convolutions\n",
    "            \n",
    "            # First Transposed Convolutional Block: Upsamples and decreases filters\n",
    "            layers.Conv2DTranspose(64, 3, strides=2, padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            \n",
    "            # Second Transposed Convolutional Block\n",
    "            layers.Conv2DTranspose(32, 3, strides=2, padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            \n",
    "            # Final Transposed Convolutional Layer: Outputs the reconstructed image (1 channel)\n",
    "            layers.Conv2DTranspose(1, 3, strides=2, padding='same', activation=last_activation),\n",
    "        ])\n",
    "    \n",
    "    # Defines the forward pass of the autoencoder\n",
    "    def call(self, input_data):\n",
    "        encoded = self.encoder(input_data) # Encode the input\n",
    "        decoded = self.decoder(encoded)    # Decode the latent representation\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### 5. Model Instantiation and Training\n",
    "\n",
    "This section handles the setup for model training, including loading previous checkpoints, compiling the model, and defining callbacks for monitoring and saving.\n",
    "\n",
    "-   **Model Pathing**: Defines paths for saving and loading the model based on hyperparameters.\n",
    "-   **Loading Checkpoint**: Checks if a pre-trained model exists at `input_model_path`. If so, it loads it and evaluates it on a small test set.\n",
    "-   **Model Compilation**: If no model is loaded, a new `Autoencoder` instance is created and compiled. The `adam` optimizer is used, `MeanSquaredError` (MSE) as the loss function (suitable for regression tasks like image reconstruction), and `Accuracy` as a metric (though MSE itself is often the primary metric for regression).\n",
    "-   **Callbacks**:\n",
    "    -   `ModelCheckpoint`: Saves the model with the best validation loss.\n",
    "    -   `TensorBoard`: Logs training metrics and graph information for visualization in TensorBoard.\n",
    "-   **Training**: The `fit` method starts the training process using the `train_dataset` and validates on `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Model instantiation\n",
    "\n",
    "# Define paths for saving/loading the model and TensorBoard logs\n",
    "input_epoch = 0 # Starting epoch for loading a pre-trained model (if any)\n",
    "input_model_path = f\"0.01k-{input_epoch}Epochs-32Lat-{batch_size}Batch-{last_activation}.keras\"\n",
    "output_epoch = num_epochs # Total number of epochs for training\n",
    "output_model_path = f\"0.01k-{output_epoch}Epochs-32Lat-{batch_size}Batch-{last_activation}.keras\"\n",
    "log_dir = f\"logs/fit/v2-autoencoder-0.01k-32Lat-{last_activation}-{batch_size}Batch\"\n",
    "\n",
    "# Check if a pre-trained model exists\n",
    "if os.path.exists(input_model_path):\n",
    "    print(\"\\033[92mModel loaded. With evaluation:\\033[0m\")\n",
    "    # Load the model with custom_objects to recognize the Autoencoder class\n",
    "    autoencoder = load_model(input_model_path, custom_objects={'Autoencoder': Autoencoder})\n",
    "    autoencoder.evaluate(test_dataset, verbose=2) # Evaluate the loaded model\n",
    "    print(autoencoder.optimizer.get_config()) # Print optimizer configuration\n",
    "\n",
    "else:\n",
    "    print(\"Model not loaded\")\n",
    "    autoencoder = Autoencoder() # Create a new Autoencoder instance\n",
    "    # Compile the model: Adam optimizer, Mean Squared Error loss, and Accuracy metric\n",
    "    autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError(), metrics=[tf.keras.metrics.Accuracy()])\n",
    "\n",
    "# You can uncomment these lines to view the summary of encoder and decoder architectures\n",
    "# autoencoder.encoder.summary()\n",
    "# autoencoder.decoder.summary()\n",
    "\n",
    "## Model training\n",
    "\n",
    "# ModelCheckpoint callback: Saves the model with the best validation loss\n",
    "save_callback = tf.keras.callbacks.ModelCheckpoint(filepath=output_model_path, verbose=2, monitor='val_loss')\n",
    "\n",
    "# TensorBoard callback: Logs training progress for visualization\n",
    "tensorboard_cb = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=5,      # Save histograms of weights every 5 epochs\n",
    "    write_graph=True,      # Save the model graph definition\n",
    ")\n",
    "\n",
    "# Train the autoencoder model\n",
    "autoencoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=output_epoch,       # Total number of epochs to train for\n",
    "    initial_epoch=input_epoch, # Starting epoch (useful for resuming training)\n",
    "    validation_data=test_dataset, # Data for validation during training\n",
    "    callbacks=[save_callback, tensorboard_cb], # List of callbacks to use\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### 6. Model Testing and Visualization\n",
    "\n",
    "After training, the model's performance is evaluated on the full test dataset. Additionally, a visualization section generates predictions for a few samples from both the training and testing sets, allowing for a visual comparison of the input, predicted, and ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Model Testing\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "# Evaluate the model on the full test dataset\n",
    "print(\"Final evaluation:\")\n",
    "# Initialize results list (assuming it's defined elsewhere, e.g., `results = []`)\n",
    "results = [] # Initialize results list here for standalone execution\n",
    "ev = autoencoder.evaluate(test_dataset_full, verbose=2)\n",
    "# Append results (batch size, last activation, and loss value)\n",
    "results.append((batch_size, last_activation, ev[0])) \n",
    "\n",
    "print(\"Final evaluation result:\", ev)\n",
    "    \n",
    "print(results)\n",
    "\n",
    "# Number of samples to visualize from each dataset\n",
    "num_tests = 5\n",
    "\n",
    "# Loop through training and testing datasets for visualization\n",
    "for name, dataset in [(\"Training set\", train_dataset), (\"Testing set\", test_dataset)]:\n",
    "    # 1) Extract a few samples\n",
    "    x_input = []\n",
    "    x_real  = []\n",
    "    for input_batch, real_batch in dataset.take(num_tests):\n",
    "        x_input.extend(input_batch)\n",
    "        x_real.extend(real_batch)\n",
    "    # Stack samples into tensors for batch processing\n",
    "    x_input = tf.stack(x_input)\n",
    "    x_real  = tf.stack(x_real)\n",
    "\n",
    "    # 2) Encode and decode the input images to get predictions\n",
    "    encoded_imgs = autoencoder.encoder(x_input).numpy()\n",
    "    decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "    # 3) Plot the original input, predicted output, and target real image\n",
    "    plt.figure(figsize=(6, num_tests * 2)) # Adjust figure size based on number of samples\n",
    "    plt.suptitle(name, fontsize=16) # Set title for the plot set (Training/Testing set)\n",
    "    \n",
    "    for i in range(num_tests):\n",
    "        # Column 1: Original input image (IR)\n",
    "        ax = plt.subplot(num_tests, 3, i*3 + 1)\n",
    "        plt.imshow(x_input[i].numpy(), cmap=\"gray\")\n",
    "        ax.set_title(\"Input (IR)\")\n",
    "        ax.axis(\"off\") # Hide axes\n",
    "\n",
    "        # Column 2: Reconstructed/Predicted image (PMR estimate)\n",
    "        ax = plt.subplot(num_tests, 3, i*3 + 2)\n",
    "        plt.imshow(decoded_imgs[i], cmap=\"gray\")\n",
    "        ax.set_title(\"Prediction (PMR)\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Column 3: \"Real\" target image (Ground Truth PMR)\n",
    "        ax = plt.subplot(num_tests, 3, i*3 + 3)\n",
    "        plt.imshow(x_real[i].numpy(), cmap=\"gray\")\n",
    "        ax.set_title(\"Target (PMR)\")\n",
    "        ax.axis(\"off\")\n",
    "                \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show() # Display all generated plots\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}