{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee21d229",
      "metadata": {},
      "source": [
        "# GAN Experimentation and Results Analysis - Interactive Notebook\n",
        "\n",
        "This notebook provides an interactive way to understand, test, and visualize components of the GAN experimentation pipeline (`test_models.py`, `metrics_calculator.py`, `plot_results.py`).\n",
        "\n",
        "**Instructions:**\n",
        "1.  Ensure you have the necessary Python scripts in the same directory or accessible in your Python path.\n",
        "2.  Modify paths and parameters in the code cells as needed.\n",
        "3.  Execute cells sequentially or as desired to explore different functionalities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d63a343",
      "metadata": {},
      "source": [
        "## Setup: Import Libraries and Define Common Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c239390b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Essential Imports ---\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Potentially from your scripts (add as needed) ---\n",
        "# from metrics_calculator import calculate_all_metrics, calculate_fid_metric # etc.\n",
        "# from plot_results import plot_experiment_results # etc.\n",
        "\n",
        "# --- Common Parameters (Modify these) ---\n",
        "BASE_DIR = \".\" # Assuming scripts are in the current directory\n",
        "DATAROOT = \"./datasets/your_dataset/\" # Path to your dataset\n",
        "MODEL_TYPE = \"pix2pix\" # or \"cycle_gan\"\n",
        "DIRECTION = \"AtoB\" # or \"BtoA\"\n",
        "GPU_ID_STR = \"0\" # GPU to use, or \"-1\" for CPU\n",
        "NUM_TEST_IMAGES = 50 # Number of images to test\n",
        "\n",
        "CHECKPOINTS_BASE_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
        "RESULTS_BASE_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "METRICS_DETAILS_DIR = os.path.join(BASE_DIR, \"metrics_details\")\n",
        "PLOTS_OUTPUT_DIR = os.path.join(BASE_DIR, \"plots_notebook\")\n",
        "EXPERIMENT_LOG_CSV = os.path.join(BASE_DIR, \"experiment_log_notebook.csv\")\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(METRICS_DETAILS_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Base directory: {os.path.abspath(BASE_DIR)}\")\n",
        "print(f\"Plots will be saved to: {os.path.abspath(PLOTS_OUTPUT_DIR)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc50f00",
      "metadata": {},
      "source": [
        "## `test_models.py`: Defining Experiment Configurations\n",
        "\n",
        "Here, we can define or load example experiment configurations similar to how `test_models.py` does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d03ea3b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "example_experiment_config = {\n",
        "    'id': 'NB_Test01',\n",
        "    'lr': 0.0002,\n",
        "    'batch_size': 1,\n",
        "    'n_epochs': 1, # Keep low for quick testing\n",
        "    'n_epochs_decay': 0,\n",
        "    'netG': 'unet_256',\n",
        "    'netD': 'basic',\n",
        "    'norm': 'batch',\n",
        "    'preprocess': 'resize_and_crop',\n",
        "    'load_size': 256,\n",
        "    'crop_size': 256,\n",
        "    'no_flip': False,\n",
        "    'model_type': MODEL_TYPE,\n",
        "    'direction': DIRECTION,\n",
        "    'dataroot': DATAROOT,\n",
        "    'gpu_ids': GPU_ID_STR\n",
        "}\n",
        "\n",
        "print(\"Example Experiment Configuration:\")\n",
        "print(json.dumps(example_experiment_config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3188d19",
      "metadata": {},
      "source": [
        "## `test_models.py`: Simulating an Experiment Run (Command Generation)\n",
        "\n",
        "This cell demonstrates how commands might be generated. To actually run them, you'd typically use `subprocess.run()` as in `test_models.py`. For safety in a notebook, we'll just print the commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d557de9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_train_command(config):\n",
        "    cmd = [\n",
        "        \"python\", \"train.py\",\n",
        "        \"--dataroot\", str(config['dataroot']),\n",
        "        \"--name\", f\"{config['model_type']}_{config['id']}\",\n",
        "        \"--model\", str(config['model_type']),\n",
        "        \"--direction\", str(config['direction']),\n",
        "        \"--gpu_ids\", str(config['gpu_ids']),\n",
        "        \"--lr\", str(config['lr']),\n",
        "        \"--batch_size\", str(config['batch_size']),\n",
        "        \"--n_epochs\", str(config['n_epochs']),\n",
        "        \"--n_epochs_decay\", str(config['n_epochs_decay']),\n",
        "        \"--netG\", str(config['netG']),\n",
        "        \"--netD\", str(config['netD']),\n",
        "        \"--norm\", str(config['norm']),\n",
        "        \"--preprocess\", str(config['preprocess']),\n",
        "        \"--load_size\", str(config['load_size']),\n",
        "        \"--crop_size\", str(config['crop_size']),\n",
        "        \"--checkpoints_dir\", CHECKPOINTS_BASE_DIR\n",
        "    ]\n",
        "    if config.get('no_flip'):\n",
        "        cmd.append(\"--no_flip\")\n",
        "    return cmd\n",
        "\n",
        "def generate_test_command(config):\n",
        "    cmd = [\n",
        "        \"python\", \"test.py\",\n",
        "        \"--dataroot\", str(config['dataroot']),\n",
        "        \"--name\", f\"{config['model_type']}_{config['id']}\",\n",
        "        \"--model\", str(config['model_type']),\n",
        "        \"--direction\", str(config['direction']),\n",
        "        \"--gpu_ids\", str(config['gpu_ids']),\n",
        "        \"--netG\", str(config['netG']),\n",
        "        \"--norm\", str(config['norm']),\n",
        "        \"--preprocess\", str(config['preprocess']),\n",
        "        \"--load_size\", str(config['load_size']),\n",
        "        \"--crop_size\", str(config['crop_size']),\n",
        "        \"--results_dir\", RESULTS_BASE_DIR,\n",
        "        \"--checkpoints_dir\", CHECKPOINTS_BASE_DIR,\n",
        "        \"--num_test\", str(NUM_TEST_IMAGES),\n",
        "        \"--eval\" # Important for consistent naming for metrics\n",
        "    ]\n",
        "    return cmd\n",
        "\n",
        "train_cmd_str = \" \".join(generate_train_command(example_experiment_config))\n",
        "test_cmd_str = \" \".join(generate_test_command(example_experiment_config))\n",
        "\n",
        "print(\"--- Example Training Command ---\")\n",
        "print(train_cmd_str)\n",
        "print(\"\\n--- Example Testing Command ---\")\n",
        "print(test_cmd_str)\n",
        "\n",
        "# To actually run (use with caution, ensure paths and scripts are correct):\n",
        "# print(\"\\n--- Running Training (Example - Will take time) ---\")\n",
        "# subprocess.run(generate_train_command(example_experiment_config), check=True)\n",
        "# print(\"\\n--- Running Testing (Example) ---\")\n",
        "# subprocess.run(generate_test_command(example_experiment_config), check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57cab1bd",
      "metadata": {},
      "source": [
        "## `test_models.py`: Logging Experiment Results (CSV and JSON)\n",
        "\n",
        "Simulate logging results to a CSV and saving MSE scores to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a812b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate some results after an experiment run\n",
        "mock_results = {\n",
        "    'exp_id': example_experiment_config['id'],\n",
        "    'full_exp_name': f\"{example_experiment_config['model_type']}_{example_experiment_config['id']}\",\n",
        "    'lr': example_experiment_config['lr'],\n",
        "    'batch_size': example_experiment_config['batch_size'],\n",
        "    # ... other params from config ...\n",
        "    'avg_psnr': 25.5,\n",
        "    'avg_ssim': 0.85,\n",
        "    'fid': 50.1,\n",
        "    'avg_lpips': 0.1,\n",
        "    'avg_mse': 0.01\n",
        "}\n",
        "\n",
        "mock_mse_scores = np.random.rand(NUM_TEST_IMAGES) * 0.05 # Example MSE scores\n",
        "\n",
        "# --- Log to CSV ---\n",
        "df_log = pd.DataFrame([mock_results])\n",
        "if not os.path.exists(EXPERIMENT_LOG_CSV):\n",
        "    df_log.to_csv(EXPERIMENT_LOG_CSV, index=False)\n",
        "else:\n",
        "    df_log.to_csv(EXPERIMENT_LOG_CSV, mode='a', header=False, index=False)\n",
        "print(f\"Appended results to {EXPERIMENT_LOG_CSV}\")\n",
        "display(pd.read_csv(EXPERIMENT_LOG_CSV).tail())\n",
        "\n",
        "# --- Save MSE scores to JSON ---\n",
        "mse_json_path = os.path.join(METRICS_DETAILS_DIR, f\"{mock_results['exp_id']}_mse_scores.json\")\n",
        "with open(mse_json_path, 'w') as f:\n",
        "    json.dump(mock_mse_scores.tolist(), f)\n",
        "print(f\"Saved MSE scores to {mse_json_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74471cb0",
      "metadata": {},
      "source": [
        "## `metrics_calculator.py`: Calculating Metrics\n",
        "\n",
        "Here you would call functions from your `metrics_calculator.py` script. We'll use placeholders if the script/dependencies are not directly available in the notebook's environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572090bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_psnr_ssim_mse_for_images_notebook_example(gen_img_array, real_img_array):\n",
        "    \"\"\"Placeholder: Replace with actual skimage.metrics calls.\"\"\"\n",
        "    # Assuming images are numpy arrays, normalized [0,1]\n",
        "    mse = np.mean((real_img_array - gen_img_array) ** 2)\n",
        "    if mse == 0:\n",
        "        psnr = float('inf')\n",
        "    else:\n",
        "        data_range = 1.0\n",
        "        psnr = 20 * np.log10(data_range / np.sqrt(mse))\n",
        "    ssim = 0.9 # Dummy value\n",
        "    print(f\"Placeholder PSNR: {psnr:.2f}, SSIM: {ssim:.2f}, MSE: {mse:.4f}\")\n",
        "    return psnr, ssim, mse\n",
        "\n",
        "# Example: Create dummy image data (replace with actual image loading)\n",
        "dummy_real_img = np.random.rand(256, 256, 3)\n",
        "dummy_gen_img = np.clip(dummy_real_img + np.random.normal(0, 0.1, (256,256,3)), 0, 1)\n",
        "\n",
        "calculate_psnr_ssim_mse_for_images_notebook_example(dummy_gen_img, dummy_real_img)\n",
        "\n",
        "# --- For LPIPS and FID, you'd typically call your script's functions ---\n",
        "# output_images_dir_for_metrics = os.path.join(RESULTS_BASE_DIR, f\"{MODEL_TYPE}_{example_experiment_config['id']}\", \"test_latest\", \"images\")\n",
        "# if os.path.exists(output_images_dir_for_metrics):\n",
        "#     avg_p, avg_s, avg_l, avg_m, all_m = calculate_all_metrics(output_images_dir_for_metrics, direction=DIRECTION)\n",
        "#     print(f\"Calculated All Metrics: PSNR={avg_p:.2f}, SSIM={avg_s:.2f}, LPIPS={avg_l:.3f}, MSE={avg_m:.4f}\")\n",
        "#     fid_score = calculate_fid_metric(output_images_dir_for_metrics, direction=DIRECTION)\n",
        "#     print(f\"Calculated FID: {fid_score:.2f}\")\n",
        "# else:\n",
        "#     print(f\"Image directory for metrics not found: {output_images_dir_for_metrics}\")\n",
        "print(\"\\nLPIPS and FID calculation would require actual images and dependencies.\")\n",
        "print(\"Uncomment and adapt above lines if you have run an experiment and have output images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95a4095b",
      "metadata": {},
      "source": [
        "## `plot_results.py`: Loading Data and Generating Plots\n",
        "\n",
        "Load the experiment log and metrics details to generate visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a351076",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Load Experiment Log ---\n",
        "if os.path.exists(EXPERIMENT_LOG_CSV):\n",
        "    df_results = pd.read_csv(EXPERIMENT_LOG_CSV)\n",
        "    print(\"Loaded experiment log:\")\n",
        "    display(df_results.head())\n",
        "else:\n",
        "    print(f\"Experiment log {EXPERIMENT_LOG_CSV} not found. Run previous cells to create it.\")\n",
        "    df_results = pd.DataFrame() # Empty dataframe\n",
        "\n",
        "# --- Load MSE scores for a specific experiment (if available) ---\n",
        "exp_id_to_plot = example_experiment_config['id'] # From earlier cell\n",
        "mse_scores_path_to_load = os.path.join(METRICS_DETAILS_DIR, f\"{exp_id_to_plot}_mse_scores.json\")\n",
        "loaded_mse_scores = []\n",
        "if os.path.exists(mse_scores_path_to_load):\n",
        "    with open(mse_scores_path_to_load, 'r') as f:\n",
        "        loaded_mse_scores = json.load(f)\n",
        "    print(f\"\\nLoaded {len(loaded_mse_scores)} MSE scores for experiment {exp_id_to_plot}.\")\n",
        "else:\n",
        "    print(f\"\\nMSE scores file not found: {mse_scores_path_to_load}\")\n",
        "    loaded_mse_scores = mock_mse_scores.tolist() # Use mock if not found for plotting demo\n",
        "    print(f\"Using mock MSE scores for plotting demo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f49d2f",
      "metadata": {},
      "source": [
        "### Plotting: Bar Plot for Overall Metric Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91294637",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df_results.empty:\n",
        "    # Ensure numeric types for plotting\n",
        "    metrics_to_plot = ['avg_psnr', 'avg_ssim', 'fid', 'avg_lpips', 'avg_mse']\n",
        "    for metric in metrics_to_plot:\n",
        "        if metric in df_results.columns:\n",
        "            df_results[metric] = pd.to_numeric(df_results[metric], errors='coerce')\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Plotting avg_mse as an example\n",
        "    if 'avg_mse' in df_results.columns and not df_results['avg_mse'].isnull().all():\n",
        "        sns.barplot(x='exp_id', y='avg_mse', data=df_results)\n",
        "        plt.title('Average MSE Comparison Across Experiments')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('Average MSE')\n",
        "        plt.xlabel('Experiment ID')\n",
        "        plt.tight_layout()\n",
        "        plot_save_path = os.path.join(PLOTS_OUTPUT_DIR, \"bar_avg_mse_comparison.png\")\n",
        "        plt.savefig(plot_save_path)\n",
        "        plt.show()\n",
        "        print(f\"Saved bar plot to {plot_save_path}\")\n",
        "    else:\n",
        "        print(\"Skipping avg_mse bar plot: No data or all NaN.\")\n",
        "else:\n",
        "    print(\"DataFrame is empty, skipping bar plot.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee982b98",
      "metadata": {},
      "source": [
        "### Plotting: MSE Distributions (Boxplot and Histogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1968a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "if loaded_mse_scores:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Boxplot\n",
        "    axes[0].boxplot(loaded_mse_scores, vert=True)\n",
        "    axes[0].set_title(f'Boxplot of MSE Scores\\n{exp_id_to_plot}')\n",
        "    axes[0].set_ylabel('MSE')\n",
        "    axes[0].set_xticks([])\n",
        "\n",
        "    # Histogram\n",
        "    axes[1].hist(loaded_mse_scores, bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[1].set_title(f'Histogram of MSE Scores\\n{exp_id_to_plot}')\n",
        "    axes[1].set_xlabel('MSE')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plot_save_path = os.path.join(PLOTS_OUTPUT_DIR, f\"mse_distribution_{exp_id_to_plot}.png\")\n",
        "    plt.savefig(plot_save_path)\n",
        "    plt.show()\n",
        "    print(f\"Saved MSE distribution plot to {plot_save_path}\")\n",
        "else:\n",
        "    print(\"No MSE scores loaded to plot distributions.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
